{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d625e0f",
   "metadata": {},
   "source": [
    "# Mecab\n",
    "\n",
    "- 自分で単語を追加することもできる。\n",
    "\n",
    "- install手順\n",
    "- [MacにMecabを入れて形態素解析をする](https://yoshikiito.net/blog/archives/mac-mecab/)\n",
    "```\n",
    "brew install mecab\n",
    "brew install mecab-ipadic\n",
    "pip install mecab-python3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c67d576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ccdd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "メイ\t名詞,一般,*,*,*,*,*\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "恋\t名詞,一般,*,*,*,*,恋,コイ,コイ\n",
      "ダンス\t名詞,サ変接続,*,*,*,*,ダンス,ダンス,ダンス\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "踊っ\t動詞,自立,*,*,五段・ラ行,連用タ接続,踊る,オドッ,オドッ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "いる\t動詞,非自立,*,*,一段,基本形,いる,イル,イル\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MeCabオブジェクトの生成 --- (*1)\n",
    "tagger = MeCab.Tagger()\n",
    "# 形態素解析 --- (*2)\n",
    "result = tagger.parse(\"メイが恋ダンスを踊っている。\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bca5f0c",
   "metadata": {},
   "source": [
    "# mecab-ipadic-neologd\n",
    "\n",
    "- インストール手順\n",
    "- [【Ubuntu】MeCabとNEologdをインストールしてPythonで形態素解析する](https://engineeeer.com/mecab-neologd-python/)\n",
    "```\n",
    "git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
    "cd mecab-ipadic-neologd\n",
    "./bin/install-mecab-ipadic-neologd -n\n",
    "echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"\n",
    "```\n",
    "\n",
    "```\n",
    "/opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f488d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "メイ\t名詞,固有名詞,人名,一般,*,*,M.A.Y,メイ,メイ\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "恋ダンス\t名詞,固有名詞,一般,*,*,*,恋ダンス,コイダンス,コイダンス\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "踊っ\t動詞,自立,*,*,五段・ラ行,連用タ接続,踊る,オドッ,オドッ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "いる\t動詞,非自立,*,*,一段,基本形,いる,イル,イル\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mecab-ipadic-NEologd辞書を指定して、MeCabオブジェクトの生成 --- (*1)\n",
    "tagger = MeCab.Tagger(\"-d /opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "# 形態素解析\n",
    "result = tagger.parse(\"メイが恋ダンスを踊っている。\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ec92cd",
   "metadata": {},
   "source": [
    "# ストップワードの除去\n",
    "\n",
    "- 助詞や助動詞などの利用頻度の高いワードを除去する。\n",
    "- 以下は品詞情報を用いて除去する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6698182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['メイ', '恋ダンス', '踊る', 'いる']\n"
     ]
    }
   ],
   "source": [
    "tagger = MeCab.Tagger(\"-d /opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "tagger.parse(\"\") \n",
    "# 形態素解析結果をリストで取得 --- (*1)\n",
    "node = tagger.parseToNode(\"メイが恋ダンスを踊っている。\")\n",
    "\n",
    "result = []\n",
    "while node is not None:\n",
    "    # 品詞情報取得 --- (*2)\n",
    "    hinshi = node.feature.split(\",\")[0]\n",
    "    if  hinshi in [\"名詞\"]:\n",
    "        # 表層形の取得 --- (*3)\n",
    "        result.append(node.surface)\n",
    "    elif hinshi in [\"動詞\", \"形容詞\"]:\n",
    "        # 形態素情報から原形情報を取得 --- (*4)\n",
    "        # 品詞が動詞や形容詞の場合、送りがななどに違いが出るため、ストップワードの除去とは関係してないが、原型を取得することで正規化。\n",
    "        result.append(node.feature.split(\",\")[6])\n",
    "    node = node.next\n",
    "    \n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaadd66",
   "metadata": {},
   "source": [
    "# 単語ベクトル\n",
    "\n",
    "- 単語の意味を計算。\n",
    "- 単語の類似度を計算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c36d592",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "- 単語をベクトル化できる。\n",
    "- 「単語はその周りにある単語と関係がある」ということを学習する。\n",
    "- 反対の意味を持ちながら、同じように使われてしまう単語が近いベクトルを持つという弱点がある。\n",
    "- 「Skip-gram」と「CBOW」という２つのアルゴリズムが存在する。\n",
    "- Skip-gram\n",
    "    - 精度が高く速度が遅い。\n",
    "- CBOW\n",
    "    - 精度が比較的低く、速度が速い。\n",
    "    \n",
    "- 何が自分の目的に合うかを考慮して「学習させる文章」を選ぶ必要がある。\n",
    "\n",
    "```\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f0426d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85774295",
   "metadata": {},
   "source": [
    "# Doc2Vec\n",
    "- 任意の長さの文章をベクトル化できる。\n",
    "- Word2Vecよりも複雑な計算をするので、より多くのメモリが必要になる。\n",
    "\n",
    "今回のプログラムを応用することで、スパム判定、問い合わせの分類などもできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b1a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os.path\n",
    "import urllib.request as req\n",
    "import MeCab\n",
    "from gensim import models\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1efc875c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデル作成完了\n"
     ]
    }
   ],
   "source": [
    "#Mecabの初期化\n",
    "mecab = MeCab.Tagger()\n",
    "mecab.parse(\"\")\n",
    "\n",
    "#学習対象とする青空文庫の作品リスト --- (*1)\n",
    "# ZIPファイルをダウンロード\n",
    "# 4人の作家のそれぞれ5作品ずつダウンロードするための情報を用意。\n",
    "list = [\n",
    "    {\"auther\":{\n",
    "        \"name\":\"宮澤 賢治\",\n",
    "        \"url\":\"https://www.aozora.gr.jp/cards/000081/files/\"}, \n",
    "     \"books\":[\n",
    "        {\"name\":\"銀河鉄道の夜\",\"zipname\":\"43737_ruby_19028.zip\"},\n",
    "        {\"name\":\"注文の多い料理店\",\"zipname\":\"1927_ruby_17835.zip\"},\n",
    "        {\"name\":\"セロ弾きのゴーシュ\",\"zipname\":\"470_ruby_3987.zip\"},\n",
    "        {\"name\":\"やまなし\",\"zipname\":\"46605_ruby_29758.zip\"},\n",
    "        {\"name\":\"どんぐりと山猫\",\"zipname\":\"43752_ruby_17595.zip\"},\n",
    "    ]},\n",
    "    {\"auther\":{\n",
    "        \"name\":\"芥川 竜之介\",\n",
    "        \"url\":\"https://www.aozora.gr.jp/cards/000879/files/\"}, \n",
    "     \"books\":[\n",
    "        {\"name\":\"羅生門\",\"zipname\":\"127_ruby_150.zip\"},\n",
    "        {\"name\":\"鼻\",\"zipname\":\"42_ruby_154.zip\"},\n",
    "        {\"name\":\"河童\",\"zipname\":\"69_ruby_1321.zip\"},\n",
    "        {\"name\":\"歯車\",\"zipname\":\"42377_ruby_34744.zip\"},\n",
    "        {\"name\":\"老年\",\"zipname\":\"131_ruby_241.zip\"},\n",
    "    ]},\n",
    "    {\"auther\":{\n",
    "        \"name\":\"ポー エドガー・アラン\",\n",
    "        \"url\":\"https://www.aozora.gr.jp/cards/000094/files/\"}, \n",
    "     \"books\":[\n",
    "        {\"name\":\"ウィリアム・ウィルスン\",\"zipname\":\"2523_ruby_19896.zip\"},\n",
    "        {\"name\":\"落穴と振子\",\"zipname\":\"1871_ruby_17551.zip\"},\n",
    "        {\"name\":\"黒猫\",\"zipname\":\"530_ruby_20931.zip\"},\n",
    "        {\"name\":\"群集の人\",\"zipname\":\"56535_ruby_69925.zip\"},\n",
    "        {\"name\":\"沈黙\",\"zipname\":\"56537_ruby_70425.zip\"},\n",
    "    ]},\n",
    "    {\"auther\":{\n",
    "        \"name\":\"紫式部\",\n",
    "        \"url\":\"https://www.aozora.gr.jp/cards/000052/files/\"}, \n",
    "     \"books\":[\n",
    "        {\"name\":\"源氏物語 01 桐壺\",\"zipname\":\"5016_ruby_9746.zip\"},\n",
    "        {\"name\":\"源氏物語 02 帚木\",\"zipname\":\"5017_ruby_9752.zip\"},\n",
    "        {\"name\":\"源氏物語 03 空蝉\",\"zipname\":\"5018_ruby_9754.zip\"},\n",
    "        {\"name\":\"源氏物語 04 夕顔\",\"zipname\":\"5019_ruby_9761.zip\"},\n",
    "        {\"name\":\"源氏物語 05 若紫\",\"zipname\":\"5020_ruby_11253.zip\"},\n",
    "    ]},\n",
    "]\n",
    "\n",
    "#作品リストを取得してループ処理に渡す --- (*2)\n",
    "# 「著者と作品」という単位で返す関数を定義。\n",
    "def book_list():\n",
    "    for novelist in list:\n",
    "        auther = novelist[\"auther\"]\n",
    "        for book in novelist[\"books\"]:\n",
    "            yield auther, book\n",
    "        \n",
    "#Zipファイルを開き、中の文書を取得する --- (*3)\n",
    "# テキストファイルを文字列にして返す。\n",
    "def read_book(auther, book):\n",
    "    zipname = book[\"zipname\"]\n",
    "    #Zipファイルが無ければ取得する\n",
    "    if not os.path.exists(zipname):\n",
    "        req.urlretrieve(auther[\"url\"] + zipname, zipname)\n",
    "    zipname = book[\"zipname\"]\n",
    "    #Zipファイルを開く\n",
    "    with zipfile.ZipFile(zipname,\"r\") as zf:\n",
    "        #Zipファイルに含まれるファイルを開く。\n",
    "        for filename in zf.namelist():\n",
    "            # テキストファイル以外は処理をスキップ\n",
    "            if os.path.splitext(filename)[1] != \".txt\":\n",
    "                continue\n",
    "            with zf.open(filename,\"r\") as f: \n",
    "                #今回読むファイルはShift-JISなので指定してデコードする\n",
    "                return f.read().decode(\"shift-jis\")\n",
    "\n",
    "#引数のテキストを分かち書きして配列にする ---(*4)\n",
    "def split_words(text):\n",
    "    node = mecab.parseToNode(text)\n",
    "    wakati_words = []\n",
    "    while node is not None:\n",
    "        hinshi = node.feature.split(\",\")[0]\n",
    "        if  hinshi in [\"名詞\"]:\n",
    "            wakati_words.append(node.surface)\n",
    "        elif hinshi in [\"動詞\", \"形容詞\"]:\n",
    "            wakati_words.append(node.feature.split(\",\")[6])\n",
    "        node = node.next\n",
    "    return wakati_words\n",
    "\n",
    "#作品リストをDoc2Vecが読めるTaggedDocument形式にし、配列に追加する --- (*5)\n",
    "documents = []\n",
    "#作品リストをループで回す\n",
    "for auther, book in book_list():\n",
    "    #作品の文字列を取得\n",
    "    words = read_book(auther, book)\n",
    "    #作品の文字列を分かち書きに\n",
    "    wakati_words = split_words(words)\n",
    "    #TaggedDocumentの作成　文書=分かち書きにした作品　タグ=作者:作品名\n",
    "    document = TaggedDocument(\n",
    "        wakati_words, [auther[\"name\"] + \":\" + book[\"name\"]])\n",
    "    documents.append(document)\n",
    "    \n",
    "#TaggedDocumentの配列を使ってDoc2Vecの学習モデルを作成 --- (*6)\n",
    "'''\n",
    "dm : Doc2Vecで使うアルゴリズムを選択する。1=dmpw, 0=DBOW\n",
    "size : ベクトルの次元を設定。Doc2Vecでは基本的に300が良いとされている。\n",
    "window : 学習する単語の前後数。DBOWでは15が良いとされている。\n",
    "min_count : 最低何回出てきた文字列を対象とするかの設定。\n",
    "（今回は作家ごとに独特な言い回しなどがあると考えられるため、一回でも出てきたとき文字列を対象にする。)\n",
    "'''\n",
    "model = models.Doc2Vec(\n",
    "    documents, dm=0, vector_size=300, window=15, min_count=1)\n",
    "\n",
    "#Doc2Vecの学習モデルを保存\n",
    "model.save('aozora.model')\n",
    "\n",
    "print(\"モデル作成完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe51e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 「宮沢 賢治:よだかの星」 と似た作品は? ---\n",
      "[('宮澤 賢治:セロ弾きのゴーシュ', 0.9809406995773315), ('宮澤 賢治:注文の多い料理店', 0.9775956869125366), ('宮澤 賢治:どんぐりと山猫', 0.9771565198898315)]\n",
      "\n",
      "--- 「芥川 龍之介:犬と笛」 と似た作品は? ---\n",
      "[('芥川 竜之介:老年', 0.9604540467262268), ('芥川 竜之介:鼻', 0.9332523345947266), ('宮澤 賢治:やまなし', 0.9224491119384766)]\n",
      "\n",
      "--- 「ポー エドガー・アラン:マリー・ロジェエの怪事件」 と似た作品は? ---\n",
      "[('ポー エドガー・アラン:黒猫', 0.8875123858451843), ('ポー エドガー・アラン:ウィリアム・ウィルスン', 0.8737896084785461), ('ポー エドガー・アラン:落穴と振子', 0.8351300358772278)]\n",
      "\n",
      "--- 「紫式部:源氏物語 06 末摘花」 と似た作品は? ---\n",
      "[('紫式部:源氏物語 01 桐壺', 0.9313539862632751), ('紫式部:源氏物語 02 帚木', 0.918245255947113), ('紫式部:源氏物語 05 若紫', 0.9100158214569092)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mecabの初期化\n",
    "mecab = MeCab.Tagger()\n",
    "mecab.parse(\"\")\n",
    "\n",
    "#保存したDoc2Vec学習モデルを読み込み --- (*7)\n",
    "model = models.Doc2Vec.load('aozora.model')\n",
    "\n",
    "#分類用のZipファイルを開き、中の文書を取得する --- (*8)\n",
    "def read_book(url, zipname):\n",
    "    if not os.path.exists(zipname):\n",
    "        req.urlretrieve(url, zipname)\n",
    "\n",
    "    with zipfile.ZipFile(zipname,\"r\") as zf:\n",
    "        for filename in zf.namelist():\n",
    "            with zf.open(filename,\"r\") as f:\n",
    "                return f.read().decode(\"shift-jis\")\n",
    "\n",
    "#引数のテキストを分かち書きして配列にする\n",
    "def split_words(text):\n",
    "    node = mecab.parseToNode(text)\n",
    "    wakati_words = []\n",
    "    while node is not None:\n",
    "        hinshi = node.feature.split(\",\")[0]\n",
    "        if  hinshi in [\"名詞\"]:\n",
    "            wakati_words.append(node.surface)\n",
    "        elif hinshi in [\"動詞\", \"形容詞\"]:\n",
    "            wakati_words.append(node.feature.split(\",\")[6])\n",
    "        node = node.next\n",
    "    return wakati_words\n",
    "\n",
    "#引数のタイトル、URLの作品を分類する --- (*9)\n",
    "# もっとも類似している３作品を表示している。\n",
    "def similar(title, url):\n",
    "    zipname = url.split(\"/\")[-1]\n",
    "        \n",
    "    words = read_book(url, zipname)\n",
    "    wakati_words = split_words(words)\n",
    "    vector = model.infer_vector(wakati_words)\n",
    "    print(\"--- 「\" + title + '」 と似た作品は? ---')\n",
    "    print(model.docvecs.most_similar([vector],topn=3))\n",
    "    print(\"\")\n",
    "\n",
    "#各作家の作品を１つずつ分類 --- (*10)\n",
    "similar(\"宮沢 賢治:よだかの星\",\n",
    "        \"https://www.aozora.gr.jp/cards/000081/files/473_ruby_467.zip\")\n",
    "\n",
    "similar(\"芥川 龍之介:犬と笛\",\n",
    "        \"https://www.aozora.gr.jp/cards/000879/files/56_ruby_845.zip\")\n",
    "\n",
    "similar(\"ポー エドガー・アラン:マリー・ロジェエの怪事件\",\n",
    "        \"https://www.aozora.gr.jp/cards/000094/files/4261_ruby_54182.zip\")\n",
    "\n",
    "similar(\"紫式部:源氏物語 06 末摘花\",\n",
    "        \"https://www.aozora.gr.jp/cards/000052/files/5021_ruby_11106.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245ec06d",
   "metadata": {},
   "source": [
    "# マルコフ連鎖\n",
    "\n",
    "- 未来の状態が現在の状態のみで決まる（過去の状態とは無関係である。）という性質を持つ確率過程のこと。\n",
    "- これを利用することで、既存の文章を利用し、自動で文章を生成することができる。\n",
    "\n",
    "#### マルコフ連鎖を使った自動作文の３ステップ\n",
    "1. 入力された文章を単語に分割（形態素解析）\n",
    "2. 辞書を作成。\n",
    "    - 文章を構成する各単語について前後の結びつきを登録。\n",
    "3. 始点となる単語と辞書を使用して作文。\n",
    "    - 辞書に登録されている同じ組み合わせを持つ単語をランダムに選択して繋げる。\n",
    "    - 「魚」→「は」→「好き」というように１つの単語から次の単語を推測する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89b68251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_file = \"markov_dict.json\"\n",
    "# dic = {}\n",
    "\n",
    "# # 辞書への登録 --- (*1)\n",
    "# # 形態素解析した結果の単語リストの内容をマルコフ連鎖の辞書へ登録し、外部ファイルとして保存している。\n",
    "# # 外部ファイルに保存することで、辞書の内容を蓄積して、自動作文器を育てることができる。\n",
    "# def regist_dic(wordlist):\n",
    "#     global dic\n",
    "#     w1 = \"\"\n",
    "#     w2 = \"\"\n",
    "    \n",
    "#     # 要素が3未満の場合は、何もしない\n",
    "#     if len(wordlist) < 3 : return\n",
    "    \n",
    "#     for w in wordlist :\n",
    "#         word = w[0]\n",
    "#         if word == \"\" or  word == \"\\r\\n\" or word == \"\\n\" : continue\n",
    "#         # 辞書に単語を設定\n",
    "#         if w1 and w2 :\n",
    "#             set_dic(dic,w1, w2, word)\n",
    "#         # 文末を表す語のの場合、連鎖をクリアする\n",
    "#         if word == \"。\" or word == \"?\" or  word == \"？\" :\n",
    "#             w1 = \"\"\n",
    "#             w2 = \"\"\n",
    "#             continue\n",
    "#         # 次の前後関係を登録するために、単語をスライド\n",
    "#         w1, w2 = w2, word\n",
    "    \n",
    "#     # 辞書を保存\n",
    "#     json.dump(dic, open(dict_file,\"w\", encoding=\"utf-8\"))\n",
    "\n",
    "# # 辞書に単語を設定 --- (*2)\n",
    "# # 作成した辞書と始点となる単語から、応答分を作成している。\n",
    "# def set_dic(dic, w1, w2, w3):\n",
    "#     # 新しい単語の場合は、新しい辞書オブジェクトを作成\n",
    "#     if w1 not in dic : dic[w1] = {}\n",
    "#     if w2 not in dic[w1] : dic[w1][w2] = {}\n",
    "#     if w3 not in dic[w1][w2]: dic[w1][w2][w3] = 0\n",
    "#     # 単語の出現数をインクリメントする\n",
    "#     dic[w1][w2][w3] += 1\n",
    "\n",
    "# # 応答文の作成 --- (*3)\n",
    "# # 標準入力から文章を受け取ったのち、形態素解析を行う。\n",
    "# # 応答文については、品詞が「感動詞」の場合にはそのまま返し、品詞が「名詞」「形容詞」「動詞」で辞書に登録されている場合には、その単語を始点に応答文を作成。\n",
    "# def make_response(word):\n",
    "#     res = []\n",
    "    \n",
    "#     # 「名詞」/「形容詞」/「動詞」は、文章の意図を示していることが多いと想定し、始点の単語とする。\n",
    "#     w1 = word\n",
    "#     res.append(w1)\n",
    "#     w2 = word_choice(dic[w1])\n",
    "#     res.append(w2)\n",
    "#     while True:\n",
    "#         # w1,w2の組み合わせから予想される、単語を選択\n",
    "#         if w1 in dic and w2 in dic[w1] : w3 = word_choice(dic[w1][w2])\n",
    "#         else : w3 = \"\"\n",
    "#         res.append(w3)\n",
    "#         # 文末を表す語の場合、作文を終了\n",
    "#         if w3 == \"。\" or w3 == \"?\" or  w3 == \"？\"  or w3 == \"\" :  break\n",
    "#         # 次の単語を選択するために、単語をスライド\n",
    "#         w1, w2 = w2, w3\n",
    "#     return \"\".join(res)\n",
    "        \n",
    "# def word_choice(candidate):\n",
    "#     keys = candidate.keys()\n",
    "#     return random.choice(list(keys))\n",
    "\n",
    "# # メイン処理 --- (*4)\n",
    "\n",
    "# # 辞書がすでに存在する場合は、最初に読み込む\n",
    "# if os.path.exists(dict_file):\n",
    "#         dic = json.load(open(dict_file,\"r\"))\n",
    "        \n",
    "# while True:\n",
    "#     # 標準入力から入力を受け付け、「さようなら」が入力されるまで続ける\n",
    "#     text = input(\"You -> \")\n",
    "#     if text == \"\" or text == \"さようなら\" : \n",
    "#         print(\"Bot -> さようなら\")\n",
    "#         break\n",
    "\n",
    "#     # 文章整形\n",
    "#     if text[-1] != \"。\" and text[-1] != \"?\" and text[-1] != \"？\" : text +=\"。\"\n",
    "    \n",
    "#     # 形態素解析\n",
    "#     tagger = MeCab.Tagger(\"-d /opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "#     tagger.parse(\"\") \n",
    "#     node =  tagger.parseToNode(text)\n",
    "    \n",
    "#     # 形態素解析の結果から、単語と品詞情報を抽出\n",
    "#     wordlist = []\n",
    "#     while node is not None:\n",
    "#         hinshi = node.feature.split(\",\")[0]\n",
    "#         if  hinshi not  in [\"BOS/EOS\"]:\n",
    "#             wordlist.append([node.surface,hinshi])\n",
    "#         node = node.next\n",
    "    \n",
    "#     # マルコフ連鎖の辞書に登録\n",
    "#     regist_dic(wordlist)\n",
    "\n",
    "#     # 応答文の作成\n",
    "#     for w in wordlist:\n",
    "#         word = w[0]\n",
    "#         hinshi = w[1]\n",
    "#         # 品詞が「感動詞」の場合は、単語をそのまま返す\n",
    "#         if hinshi in [ \"感動詞\"] : \n",
    "#             print(\"Bot -> \" + word)\n",
    "#             break\n",
    "#         # 品詞が「名詞」「形容詞」「動詞」の場合で、かつ、辞書に単語が存在する場合は、作文して返す\n",
    "#         elif (hinshi in [ \"名詞\" ,\"形容詞\",\"動詞\"]) and (word in dic):\n",
    "#             print(\"Bot -> \" + make_response(word))\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd4f4b",
   "metadata": {},
   "source": [
    "# ベイジアンフィルター\n",
    "- 単純ベイズ分類器を応用したもの。\n",
    "- 統計的な手法でスパムを判定。\n",
    "\n",
    "#### テキストデータの学習方法\n",
    "- Bowの手法を用いた手順。\n",
    "1. テキストを形態素解析して単語に分ける。\n",
    "    - 「ネコに小判と言うがネコにはネコの世界がある」\n",
    "    - ネコ | に | 小判 | と | 言う | が | ネコ | に | は | ネコ | の | 世界 | が | ある |\n",
    "2. ストップワードを取り除く\n",
    "    - ネコ | 小判 | 言う | ネコ | ネコ | 世界 | ある |\n",
    "3. 単語辞書を作り、単語にIDを振る\n",
    "\n",
    "| 単語 | ID |\n",
    "| ---- | ---- |\n",
    "| ネコ | 0 |\n",
    "| 小判 | 1 |\n",
    "| 言う | 2 |\n",
    "| 世界 | 3 |\n",
    "| ある | 4 |\n",
    "\n",
    "4. ファイルごとの単語の出現頻度を調べる\n",
    "\n",
    "| ID | 出現回数 | 出現頻度 |\n",
    "| ---- | ---- | ---- |\n",
    "| 0 | 3 | 0.43 |\n",
    "| 1 | 1 | 0.14 |\n",
    "| 2 | 1 | 0.14 |\n",
    "| 3 | 1 | 0.14 |\n",
    "| 4 | 1 | 0.14 |\n",
    "\n",
    "5. 単語の出現頻度データをもとにok(0)とspam(1)に分けて学習。\n",
    "\n",
    "- スパムテキスト → [https://github.com/kujirahand/spam-database-ja](https://github.com/kujirahand/spam-database-ja)\n",
    "- 非スパムテキスト → [ldcc-20140209.tar.gz](https://www.rondhuit.com/download.html#ldcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3507b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# # 保存ファイル名\n",
    "# savefile = \"ok-spam.pickle\"\n",
    "# # MeCabの準備 --- (*1)\n",
    "# tagger = MeCab.Tagger()\n",
    "# # ファイル全体で利用する変数の準備 --- (*2)\n",
    "# word_dic = {\"__id\": 0} # 単語辞書。単語とIDを記録。word_dic[\"__id\"]にIDを発行した単語の個数を記録し、未知語に新しいID番号を記録している。\n",
    "# files = [] # 読み込んだ単語データを追加する。ファイルを読み込み、IDに変換した単語リストとラベルを追加。\n",
    "\n",
    "# # 指定したディレクトリ内のファイル一覧を読む --- (*3)\n",
    "# def read_files(dir, label):\n",
    "#     # テキストファイルの一覧を得る\n",
    "#     files = glob.glob(dir + '/*.txt')\n",
    "#     for f in files:\n",
    "#         read_file(f, label)\n",
    "\n",
    "# # ファイルを読む --- (*4)\n",
    "# # 指定されたファイルを読み込んで、文章を単語IDのリストに変換。\n",
    "# def read_file(filename, label):\n",
    "#     words = []\n",
    "#     # ファイルの内容を読む\n",
    "#     with open(filename, \"rt\", encoding=\"utf-8\") as f:\n",
    "#         text = f.read()\n",
    "#     files.append({\n",
    "#         \"label\": label,\n",
    "#         \"words\": text_to_ids(text)\n",
    "#     })\n",
    "\n",
    "# # テキストを単語IDのリストに変換\n",
    "# def text_to_ids(text):\n",
    "#     # 形態素解析 --- (*5)\n",
    "#     word_s = tagger.parse(text)\n",
    "#     words = []\n",
    "#     # 単語を辞書に登録 --- (*6)\n",
    "#     # 単語IDに変換。\n",
    "#     for line in word_s.split(\"\\n\"):\n",
    "#         if line == 'EOS' or line == '': continue\n",
    "#         word = line.split(\"\\t\")[0]\n",
    "#         params = line.split(\"\\t\")[4].split(\"-\")\n",
    "#         hinsi = params[0] # 品詞\n",
    "#         hinsi2 = params[1]  if len(params) > 1 else '' # 品詞の説明\n",
    "#         org = line.split(\"\\t\")[3]  # 単語の原型\n",
    "#         # 助詞・助動詞・記号・数字は捨てる --- (*7)\n",
    "#         if not (hinsi in ['名詞', '動詞', '形容詞']): continue\n",
    "#         if hinsi == '名詞' and hinsi2 == '数詞': continue\n",
    "#         # 単語をidに変換 --- (*8)\n",
    "#         id = word_to_id(org)\n",
    "#         words.append(id)\n",
    "#     return words\n",
    "\n",
    "# # 単語をidに変換 --- (*9)\n",
    "# # 変数wordに単語IDを追加。\n",
    "# def word_to_id(word):\n",
    "#     # 単語が辞書に登録されているか？\n",
    "#     if not (word in word_dic):\n",
    "#         # 登録されていないので新たにIDを割り振る\n",
    "#         id = word_dic[\"__id\"]\n",
    "#         word_dic[\"__id\"] += 1\n",
    "#         word_dic[word] = id\n",
    "#     else:\n",
    "#         # 既存の単語IDを返す\n",
    "#         id = word_dic[word]\n",
    "#     return id\n",
    "\n",
    "# # 単語の頻出頻度のデータを作る --- (*10)\n",
    "# def make_freq_data_allfiles():\n",
    "#     y = []\n",
    "#     x = []\n",
    "#     for f in files:\n",
    "#         y.append(f['label'])\n",
    "#         x.append(make_freq_data(f['words']))\n",
    "#     return y, x\n",
    "\n",
    "# def make_freq_data(words):\n",
    "#     # 単語の出現回数を調べる\n",
    "#     cnt = 0\n",
    "#     dat = np.zeros(word_dic[\"__id\"], 'float')\n",
    "#     for w in words:\n",
    "#         dat[w] += 1\n",
    "#         cnt += 1\n",
    "#     # 回数を出現頻度に直す --- (*11)\n",
    "#     # 単語ごとに出現回数をそう出現回数でわる。\n",
    "#     dat = dat / cnt\n",
    "#     return dat\n",
    "\n",
    "# # ファイルの一覧から学習用のデータベースを作る\n",
    "# read_files(\"ok\", 0)\n",
    "# read_files(\"spam\", 1)\n",
    "# y, x = make_freq_data_allfiles()\n",
    "# # ファイルにデータを保存\n",
    "# pickle.dump([y, x, word_dic], open(savefile, 'wb'))\n",
    "# print(\"単語頻出データ作成完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9efea30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # データファイルの読込 --- (*1)\n",
    "# data_file = \"./ok-spam.pickle\"\n",
    "# save_file = \"./ok-spam-model.pickle\"\n",
    "# data = pickle.load(open(data_file, \"rb\"))\n",
    "# y = data[0] # ラベル\n",
    "# x = data[1] # 単語の出現頻度\n",
    "\n",
    "# # 100回、学習とテストを繰り返す --- (*2)\n",
    "# count = 100\n",
    "# rate = 0\n",
    "# for i in range(count):\n",
    "#     # データを学習用とテスト用に分割 --- (*3)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(\n",
    "#         x, y, test_size=0.2)\n",
    "#     # 学習する --- (*4)\n",
    "#     model = GaussianNB()\n",
    "#     model.fit(x_train, y_train)\n",
    "#     # 評価する ---(*5)\n",
    "#     y_pred = model.predict(x_test)\n",
    "#     acc = accuracy_score(y_test, y_pred)\n",
    "#     # 評価結果が良ければモデルを保存 --- (*6)\n",
    "#     if acc > 0.94: pickle.dump(model, open(save_file, \"wb\"))\n",
    "#     print(acc)\n",
    "#     rate += acc\n",
    "# # 平均値を表示 --- (*7)\n",
    "# print(\"----\")\n",
    "# print(\"average=\", rate / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eabc5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import MeCab\n",
    "# import numpy as np\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# # テストするテキスト --- (※1)\n",
    "# test_text1 = \"\"\"\n",
    "# 会社から支給されているiPhoneの調子が悪いのです。\n",
    "# 修理に出すので、しばらくはアプリのテストができません。\n",
    "# \"\"\"\n",
    "# test_text2 = \"\"\"\n",
    "# 億万長者になる方法を教えます。\n",
    "# すぐに以下のアドレスに返信して。\n",
    "# \"\"\"\n",
    "# # ファイル名\n",
    "# data_file = \"./ok-spam.pickle\"\n",
    "# model_file = \"./ok-spam-model.pickle\"\n",
    "# label_names = ['OK', 'SPAM']\n",
    "# # 単語辞書を読み出す --- (※2)\n",
    "# data = pickle.load(open(data_file, \"rb\"))\n",
    "# word_dic = data[2]\n",
    "# # MeCabの準備\n",
    "# tagger = MeCab.Tagger()\n",
    "# # 学習済みモデルを読み出す --- (※3) \n",
    "# model = pickle.load(open(model_file, \"rb\"))\n",
    "\n",
    "# # テキストがスパムかどうか判定する --- (※4)\n",
    "# def check_spam(text):\n",
    "#     # テキストを単語IDのリストに変換し単語の頻出頻度を調べる\n",
    "#     zw = np.zeros(word_dic['__id'])\n",
    "#     count = 0\n",
    "#     s = tagger.parse(text)\n",
    "#     # 単語毎の回数を加算 --- (※5)\n",
    "#     for line in s.split(\"\\n\"):\n",
    "#         if line == \"EOS\": break\n",
    "#         org =  line.split(\"\\t\")[3]# 単語の原型\n",
    "#         if org in word_dic:\n",
    "#             id = word_dic[org]\n",
    "#             zw[id] += 1\n",
    "#             count += 1\n",
    "#     zw = zw / count #  --- (※6)\n",
    "#     # 予測\n",
    "#     pre = model.predict([zw])[0] #  --- (※7)\n",
    "#     print(\"- 結果=\", label_names[pre])\n",
    "\n",
    "# check_spam(test_text1)\n",
    "# check_spam(test_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72e8fa",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "- 文章を数値ベクトルに変換。\n",
    "- 単語の出現頻度に加え、文章全体における単語の重要度を考慮。\n",
    "- 文書内おける特徴的な単語を見つけることを重視。\n",
    "- 学習される全ての文書で、その単語がどのくらいの頻度で使われているか調べる。\n",
    "    - 単語の出現回数を数えるだけでなく、出現頻度の高い単語のレートを下げ、特徴的な単語のレートを高く評価する方法で、単語をベクトル化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcd0c765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 0., 0., 0., 0., 0.]), array([0.33333333, 0.42922736, 0.56438239, 0.        , 0.        ,\n",
      "       0.        ]), array([0.2       , 0.25753641, 0.33862944, 0.47725887, 0.47725887,\n",
      "       0.        ]), array([0.33333333, 0.42922736, 0.        , 0.        , 0.        ,\n",
      "       0.79543145])]\n",
      "{'_id': 6, '雨': 0, '今日': 1, '降る': 2, '暑い': 3, '日': 4, '日曜': 5}\n"
     ]
    }
   ],
   "source": [
    "# TF-IDFでテキストをベクトル化するモジュール\n",
    "import MeCab\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# MeCabの初期化 ---- (*1)\n",
    "tagger = MeCab.Tagger(\n",
    "    \"-d /opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "# グローバル変数 --- (*2)\n",
    "word_dic = {'_id': 0} # 単語辞書\n",
    "dt_dic = {} # 文書全体での単語の出現回数\n",
    "files = [] # 全文書をIDで保存\n",
    "\n",
    "def tokenize(text):\n",
    "    '''MeCabで形態素解析を行う''' # --- (*3)\n",
    "    result = []\n",
    "    word_s = tagger.parse(text)\n",
    "    for n in word_s.split(\"\\n\"):\n",
    "        if n == 'EOS' or n == '': continue\n",
    "        p = n.split(\"\\t\")[1].split(\",\")\n",
    "        h, h2, org = (p[0], p[1], p[6])\n",
    "        if not (h in ['名詞', '動詞', '形容詞']): continue\n",
    "        if h == '名詞' and h2 == '数': continue\n",
    "        result.append(org)\n",
    "    return result\n",
    "\n",
    "def words_to_ids(words, auto_add = True):\n",
    "    '''単語一覧をIDの一覧に変換する''' # --- (*4)\n",
    "    result = []\n",
    "    for w in words:\n",
    "        if w in word_dic:\n",
    "            result.append(word_dic[w])\n",
    "            continue\n",
    "        elif auto_add:\n",
    "            id = word_dic[w] = word_dic['_id']\n",
    "            word_dic['_id'] += 1\n",
    "            result.append(id)\n",
    "    return result\n",
    "\n",
    "def add_text(text):\n",
    "    '''テキストをIDリストに変換して追加''' # --- (*5)\n",
    "    ids = words_to_ids(tokenize(text))\n",
    "    files.append(ids)\n",
    "\n",
    "def add_file(path):\n",
    "    '''テキストファイルを学習用に追加する''' # --- (*6)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        s = f.read()\n",
    "        add_text(s)\n",
    "\n",
    "def calc_files():\n",
    "    '''追加したファイルを計算''' # --- (*7)\n",
    "    global dt_dic\n",
    "    result = []\n",
    "    doc_count = len(files)\n",
    "    dt_dic = {}\n",
    "    # 単語の出現頻度を数える --- (*8)\n",
    "    for words in files:\n",
    "        used_word = {}\n",
    "        data = np.zeros(word_dic['_id'])\n",
    "        for id in words:\n",
    "            data[id] += 1\n",
    "            used_word[id] = 1\n",
    "        # 単語tが使われていればdt_dicを加算 --- (*9)\n",
    "        for id in used_word:\n",
    "            if not(id in dt_dic): dt_dic[id] = 0\n",
    "            dt_dic[id] += 1\n",
    "        # 出現回数を割合に直す --- (*10)\n",
    "        data = data / len(words) \n",
    "        result.append(data)\n",
    "    # TF-IDFを計算 --- (*11)\n",
    "    for i, doc in enumerate(result):\n",
    "        for id, v in enumerate(doc):\n",
    "            idf = np.log(doc_count / dt_dic[id]) + 1\n",
    "            doc[id] = min([doc[id] * idf, 1.0])\n",
    "        result[i] = doc\n",
    "    return result\n",
    "\n",
    "def save_dic(fname):\n",
    "    '''辞書をファイルへ保存する''' # --- (*12)\n",
    "    pickle.dump(\n",
    "        [word_dic, dt_dic, files],\n",
    "        open(fname, \"wb\"))\n",
    "\n",
    "def load_dic(fname):\n",
    "    '''辞書をファイルから読み込む''' # --- (*13)\n",
    "    global word_dic, dt_dic, files\n",
    "    n = pickle.load(open(fname, 'rb'))\n",
    "    word_dic, dt_dic, files = n\n",
    "\n",
    "def calc_text(text):\n",
    "    ''' 辞書を更新せずにベクトル変換する ''' # --- (*14)\n",
    "    data = np.zeros(word_dic['_id'])\n",
    "    words = words_to_ids(tokenize(text), False)\n",
    "    for w in words:\n",
    "        data[w] += 1\n",
    "    data = data / len(words)\n",
    "    for id, v in enumerate(data):\n",
    "        idf = np.log(len(files) / dt_dic[id]) + 1\n",
    "        data[id] = min([data[id] * idf, 1.0])\n",
    "    return data\n",
    "\n",
    "# モジュールのテスト --- (*15)\n",
    "if __name__ == '__main__':\n",
    "    add_text('雨')\n",
    "    add_text('今日は、雨が降った。')\n",
    "    add_text('今日は暑い日だったけど雨が降った。')\n",
    "    add_text('今日も雨だ。でも日曜だ。')\n",
    "    print(calc_files())\n",
    "    print(word_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87b326f",
   "metadata": {},
   "source": [
    "# 文章をTF-IDFのデータベースに変換\n",
    "\n",
    "```\n",
    "pip install tfidf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a3147fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_files= text/sports-watch\n",
      "read_files= text/it-life-hack\n",
      "read_files= text/movie-enter\n",
      "read_files= text/dokujo-tsushin\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "import os, glob, pickle\n",
    "\n",
    "# 変数の初期化\n",
    "y = []\n",
    "x = []\n",
    "\n",
    "# ディレクトリ内のファイル一覧を処理 --- (*1)\n",
    "def read_files(path, label):\n",
    "    print(\"read_files=\", path)\n",
    "    files = glob.glob(path + \"/*.txt\")\n",
    "    for f in files:\n",
    "        if os.path.basename(f) == 'LICENSE.txt': continue\n",
    "        add_file(f)\n",
    "        y.append(label)\n",
    "\n",
    "# ファイル一覧を読む --- (*2)\n",
    "read_files('text/sports-watch', 0)\n",
    "read_files('text/it-life-hack', 1)\n",
    "read_files('text/movie-enter', 2)\n",
    "read_files('text/dokujo-tsushin', 3)\n",
    "\n",
    "# TF-IDFベクトルに変換 --- (*3)\n",
    "x = calc_files()\n",
    "\n",
    "# 保存 --- (*4)\n",
    "pickle.dump([y, x], open('text/genre.pickle', 'wb'))\n",
    "save_dic('text/genre-tdidf.dic')\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46ebf6a",
   "metadata": {},
   "source": [
    "# TF-IDFをNaiveBayesで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bf0ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import sklearn.metrics as metrics\n",
    "# import numpy as np\n",
    "\n",
    "# # TF-IDFのデータベースを読み込む --- (*1)\n",
    "# data = pickle.load(open(\"text/genre.pickle\", \"rb\"))\n",
    "# y = data[0] # ラベル\n",
    "# x = data[1] # TF-IDF\n",
    "\n",
    "# # 学習用とテスト用に分ける --- (*2)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#         x, y, test_size=0.2)\n",
    "\n",
    "# # ナイーブベイズで学習 --- (*3)\n",
    "# model = GaussianNB()\n",
    "# model.fit(x_train, y_train)\n",
    "\n",
    "# # 評価して結果を出力 --- (*4)\n",
    "# y_pred = model.predict(x_test)\n",
    "# acc = metrics.accuracy_score(y_test, y_pred)\n",
    "# rep = metrics.classification_report(y_test, y_pred)\n",
    "\n",
    "# print(\"正解率=\", acc)\n",
    "# print(rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315713eb",
   "metadata": {},
   "source": [
    "# DeepLearningで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4036bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import sklearn.metrics as metrics\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from keras.optimizers import RMSprop\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import h5py\n",
    "\n",
    "# # 分類するラベルの数 --- (*1)\n",
    "# nb_classes = 4\n",
    "\n",
    "# # データベースの読込 --- (*2)\n",
    "# data = pickle.load(open(\"text/genre.pickle\", \"rb\"))\n",
    "# y = data[0] # ラベル\n",
    "# x = data[1] # TF-IDF\n",
    "# # ラベルデータをone-hotベクトルに直す --- (*3)\n",
    "# y = keras.utils.np_utils.to_categorical(y, nb_classes)\n",
    "# in_size = x[0].shape[0]\n",
    "\n",
    "# # 学習用とテスト用を分ける --- (*4)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#         np.array(x), np.array(y), test_size=0.2)\n",
    "\n",
    "# # MLPモデル構造を定義 --- (*5)\n",
    "# model = Sequential()\n",
    "# model.add(Dense(512, activation='relu', input_shape=(in_size,)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "# # モデルをコンパイル --- (*6)\n",
    "# model.compile(\n",
    "#     loss='categorical_crossentropy',\n",
    "#     optimizer=RMSprop(),\n",
    "#     metrics=['accuracy'])\n",
    "\n",
    "# # 学習を実行 --- (*7)\n",
    "# hist = model.fit(x_train, y_train,\n",
    "#           batch_size=128, \n",
    "#           epochs=20,\n",
    "#           verbose=1,\n",
    "#           validation_data=(x_test, y_test))\n",
    "\n",
    "# # 評価する ---(*8)\n",
    "# score = model.evaluate(x_test, y_test, verbose=1)\n",
    "# print(\"正解率=\", score[1], 'loss=', score[0])\n",
    "\n",
    "# # 重みデータを保存 --- (*9)\n",
    "# model.save_weights('./text/genre-model.hdf5')\n",
    "\n",
    "# # 学習の様子をグラフへ描画 --- (*10)\n",
    "# plt.plot(hist.history['accuracy'])\n",
    "# plt.plot(hist.history['val_accuracy'])\n",
    "# plt.title('Accuracy')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aecb083",
   "metadata": {},
   "source": [
    "# 自分で文章を指定して判定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dceb6df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import numpy as np\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from keras.optimizers import RMSprop\n",
    "# from keras.models import model_from_json\n",
    "\n",
    "# # 独自のテキストを指定 --- (*1)\n",
    "# text1 = \"\"\"\n",
    "# 野球を観るのは楽しいものです。\n",
    "# 試合だけでなくインタビューも楽しみです。\n",
    "# \"\"\"\n",
    "# text2 = \"\"\"\n",
    "# 常にiPhoneとiPadを持っているので、\n",
    "# 二口あるモバイルバッテリがあると便利。\n",
    "# \"\"\"\n",
    "# text3 = \"\"\"\n",
    "# 幸せな結婚の秘訣は何でしょうか。\n",
    "# 夫には敬意を、妻には愛情を示すことが大切。\n",
    "# \"\"\"\n",
    "\n",
    "# # TF-IDFの辞書を読み込む --- (*2)\n",
    "# load_dic(\"text/genre-tdidf.dic\")\n",
    "\n",
    "# # Kerasのモデルを定義して重みデータを読み込む --- (*3)\n",
    "# nb_classes = 4\n",
    "# dt_count = len(dt_dic)\n",
    "# model = Sequential()\n",
    "# model.add(Dense(512, activation='relu', input_shape=(dt_count,)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(nb_classes, activation='softmax'))\n",
    "# model.compile(\n",
    "#     loss='categorical_crossentropy',\n",
    "#     optimizer=RMSprop(),\n",
    "#     metrics=['accuracy'])\n",
    "# model.load_weights('./text/genre-model.hdf5')\n",
    "\n",
    "# # テキストを指定して判定 --- (*4)\n",
    "# def check_genre(text):\n",
    "#     # ラベルの定義\n",
    "#     LABELS = [\"スポーツ\", \"IT\", \"映画\", \"ライフ\"]\n",
    "#     # TF-IDFのベクトルに変換 -- (*5)\n",
    "#     data = calc_text(text)\n",
    "#     # MLPで予測 --- (*6)\n",
    "#     pre = model.predict(np.array([data]))[0]\n",
    "#     n = pre.argmax()\n",
    "#     print(LABELS[n], \"(\", pre[n], \")\")\n",
    "#     return LABELS[n], float(pre[n]), int(n) \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     check_genre(text1)\n",
    "#     check_genre(text2)\n",
    "#     check_genre(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc6d39",
   "metadata": {},
   "source": [
    "# Webで使える文章ジャンル判定アプリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f411ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import flask\n",
    "# from flask import request\n",
    "\n",
    "# # テキストを指定して判定 --- (*4)\n",
    "# def check_genre(text):\n",
    "#     # ラベルの定義\n",
    "#     LABELS = [\"スポーツ\", \"IT\", \"映画\", \"ライフ\"]\n",
    "#     # TF-IDFのベクトルに変換 -- (*5)\n",
    "#     data = calc_text(text)\n",
    "#     # MLPで予測 --- (*6)\n",
    "#     pre = model.predict(np.array([data]))[0]\n",
    "#     n = pre.argmax()\n",
    "#     print(LABELS[n], \"(\", pre[n], \")\")\n",
    "#     return LABELS[n], float(pre[n]), int(n) \n",
    "\n",
    "\n",
    "# # ポート番号 --- (*1)\n",
    "# TM_PORT_NO = 8888\n",
    "# # HTTPサーバを起動\n",
    "# app = flask.Flask(__name__)\n",
    "\n",
    "# # 一度、ジャンル判定のテストをする\n",
    "# label, per, no = check_genre(\"テスト\")\n",
    "# print(\"> テスト --- \", label, per, no)\n",
    "\n",
    "# # ルートへアクセスした場合 --- (*2)\n",
    "# @app.route('/', methods=['GET'])\n",
    "# def index():\n",
    "#     with open(\"index.html\", \"rb\") as f:\n",
    "#         return f.read()\n",
    "\n",
    "# # /api へアクセスした場合\n",
    "# @app.route('/api', methods=['GET'])\n",
    "# def api():\n",
    "#     # URLパラメータを取得 --- (*3)\n",
    "#     q = request.args.get('q', '')\n",
    "#     if q == '':\n",
    "#       return '{\"label\": \"空です\", \"per\":0}'\n",
    "#     print(\"q=\", q)\n",
    "#     # テキストのジャンル判定を行う --- (*4)\n",
    "#     label, per, no = check_genre(q)\n",
    "#     # 結果をJSONで出力\n",
    "#     return json.dumps({\n",
    "#       \"label\": label, \n",
    "#       \"per\": per,\n",
    "#       \"genre_no\": no\n",
    "#     })\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     # サーバを起動\n",
    "#     app.run(debug=True, host='0.0.0.0', port=TM_PORT_NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e542b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
