{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "union-walnut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/local/lib/python3.7/site-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/local/lib/python3.7/site-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/12\n",
      "40000/40000 [==============================] - 114s 3ms/step - loss: 1.7239 - accuracy: 0.3790 - val_loss: 1.5003 - val_accuracy: 0.4783\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/local/lib/python3.7/site-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/12\n",
      "40000/40000 [==============================] - 113s 3ms/step - loss: 1.3027 - accuracy: 0.5381 - val_loss: 1.2078 - val_accuracy: 0.5711\n",
      "Epoch 3/12\n",
      "40000/40000 [==============================] - 113s 3ms/step - loss: 1.1332 - accuracy: 0.6033 - val_loss: 1.1237 - val_accuracy: 0.5973\n",
      "Epoch 4/12\n",
      "40000/40000 [==============================] - 113s 3ms/step - loss: 1.0297 - accuracy: 0.6411 - val_loss: 0.9586 - val_accuracy: 0.6730\n",
      "Epoch 5/12\n",
      "40000/40000 [==============================] - 113s 3ms/step - loss: 0.9467 - accuracy: 0.6694 - val_loss: 0.9436 - val_accuracy: 0.6823\n",
      "Epoch 6/12\n",
      "40000/40000 [==============================] - 113s 3ms/step - loss: 0.8831 - accuracy: 0.6927 - val_loss: 0.9899 - val_accuracy: 0.6501\n",
      "Epoch 7/12\n",
      "40000/40000 [==============================] - 112s 3ms/step - loss: 0.8223 - accuracy: 0.7125 - val_loss: 0.8839 - val_accuracy: 0.6968\n",
      "Epoch 8/12\n",
      "40000/40000 [==============================] - 113s 3ms/step - loss: 0.7754 - accuracy: 0.7282 - val_loss: 0.8888 - val_accuracy: 0.6934\n",
      "Epoch 9/12\n",
      "40000/40000 [==============================] - 113s 3ms/step - loss: 0.7300 - accuracy: 0.7477 - val_loss: 0.8610 - val_accuracy: 0.7054\n",
      "Epoch 10/12\n",
      "40000/40000 [==============================] - 113s 3ms/step - loss: 0.6850 - accuracy: 0.7608 - val_loss: 0.8411 - val_accuracy: 0.7162\n",
      "Epoch 11/12\n",
      "40000/40000 [==============================] - 113s 3ms/step - loss: 0.6471 - accuracy: 0.7749 - val_loss: 0.8809 - val_accuracy: 0.7016\n",
      "Epoch 12/12\n",
      "40000/40000 [==============================] - 112s 3ms/step - loss: 0.6106 - accuracy: 0.7886 - val_loss: 0.8345 - val_accuracy: 0.7229\n",
      "Test loss: 0.8506066523551941\n",
      "Test accuracy: 0.7148000001907349\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten, Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "\n",
    "def network(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # extract image features by convolution and max pooling layers\n",
    "    model.add(Conv2D(\n",
    "        32, kernel_size=3, padding=\"same\",\n",
    "        input_shape=input_shape, activation=\"relu\"\n",
    "        ))   # 3x3のフィルターで32回の畳み込み\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))   # 全体の25%のノードを無視。\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"))   # 3x3のフィルターを64個使用。\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # classify the class by fully-connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))   # サイズ512のベクトルを出力。\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "class CIFAR10Dataset():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_shape = (32, 32, 3)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def get_batch(self):\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "        x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
    "        y_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
    "                           [y_train, y_test]]\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def preprocess(self, data, label_data=False):\n",
    "        if label_data:\n",
    "            # convert class vectors to binary class matrices\n",
    "            data = keras.utils.to_categorical(data, self.num_classes)\n",
    "        else:\n",
    "            data = data.astype(\"float32\")\n",
    "            data /= 255  # convert the value to 0~1 scale\n",
    "            shape = (data.shape[0],) + self.image_shape  # add dataset length\n",
    "            data = data.reshape(shape)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, loss, optimizer):\n",
    "        self._target = model\n",
    "        self._target.compile(\n",
    "            loss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "            )\n",
    "        self.verbose = 1\n",
    "        logdir = \"logdir_cifar10_net\"\n",
    "        self.log_dir = os.path.join(os.path.dirname('__file__'), logdir)\n",
    "        self.model_file_name = \"model_file.hdf5\"\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
    "        if os.path.exists(self.log_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(self.log_dir)  # remove previous execution\n",
    "        os.mkdir(self.log_dir)\n",
    "\n",
    "        model_path = os.path.join(self.log_dir, self.model_file_name)\n",
    "        self._target.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=batch_size, epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "            # 最も損失が少ないモデルを保存\n",
    "            callbacks=[\n",
    "                TensorBoard(log_dir=self.log_dir),\n",
    "                ModelCheckpoint(model_path, save_best_only=True)\n",
    "            ],\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "\n",
    "dataset = CIFAR10Dataset()\n",
    "\n",
    "# make model\n",
    "model = network(dataset.image_shape, dataset.num_classes)\n",
    "\n",
    "# train the model\n",
    "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
    "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=RMSprop())\n",
    "trainer.train(\n",
    "    x_train, y_train, batch_size=128, epochs=12, validation_split=0.2\n",
    "    )\n",
    "\n",
    "# show result\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-transcription",
   "metadata": {},
   "source": [
    "# conv + conv + maxpool + dropout + conv + conv + maxpool + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # extract image features by convolution and max pooling layers\n",
    "    model.add(Conv2D(\n",
    "        32, kernel_size=3, padding=\"same\",\n",
    "        input_shape=input_shape, activation=\"relu\"\n",
    "        ))\n",
    "    model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # classify the class by fully-connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "class CIFAR10Dataset():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_shape = (32, 32, 3)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def get_batch(self):\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "        x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
    "        y_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
    "                           [y_train, y_test]]\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def preprocess(self, data, label_data=False):\n",
    "        if label_data:\n",
    "            # convert class vectors to binary class matrices\n",
    "            data = keras.utils.to_categorical(data, self.num_classes)\n",
    "        else:\n",
    "            data = data.astype(\"float32\")\n",
    "            data /= 255  # convert the value to 0~1 scale\n",
    "            shape = (data.shape[0],) + self.image_shape  # add dataset length\n",
    "            data = data.reshape(shape)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, loss, optimizer):\n",
    "        self._target = model\n",
    "        self._target.compile(\n",
    "            loss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "            )\n",
    "        self.verbose = 1\n",
    "        logdir = \"logdir_cifar10_deep_net\"\n",
    "        self.log_dir = os.path.join(os.path.dirname('__file__'), logdir)\n",
    "        self.model_file_name = \"model_file.hdf5\"\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
    "        if os.path.exists(self.log_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(self.log_dir)  # remove previous execution\n",
    "        os.mkdir(self.log_dir)\n",
    "\n",
    "        model_path = os.path.join(self.log_dir, self.model_file_name)\n",
    "        self._target.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=batch_size, epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[\n",
    "                TensorBoard(log_dir=self.log_dir),\n",
    "                ModelCheckpoint(model_path, save_best_only=True)\n",
    "            ],\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "\n",
    "dataset = CIFAR10Dataset()\n",
    "\n",
    "# make model\n",
    "model = network(dataset.image_shape, dataset.num_classes)\n",
    "\n",
    "# train the model\n",
    "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
    "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=RMSprop())\n",
    "trainer.train(\n",
    "    x_train, y_train, batch_size=128, epochs=12, validation_split=0.2\n",
    "    )\n",
    "\n",
    "# show result\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-executive",
   "metadata": {},
   "source": [
    "# 画像を増加（Data Augmentation)\n",
    "\n",
    "#### ImageDataGenerator\n",
    "\n",
    "- rotation_range : 指定した範囲でランダムに画像の回転を行う。\n",
    "- width_shift, height_shift : 指定した範囲でランダムに横、縦に画像を動かす。\n",
    "- zoom_range : 指定した範囲でランダムに拡大縮小を行う。\n",
    "- horizontal_flip : 左右斑点をランダムに行う。\n",
    "\n",
    "#### 手順\n",
    "\n",
    "- fitにより、正規化を行う場合に必要な統計量（平均や分散など）を計算。\n",
    "- flowにより、DataAugmentationを行ったデータを生成していく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def network(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # extract image features by convolution and max pooling layers\n",
    "    model.add(Conv2D(\n",
    "        32, kernel_size=3, padding=\"same\",\n",
    "        input_shape=input_shape, activation=\"relu\"\n",
    "        ))\n",
    "    model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # classify the class by fully-connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "class CIFAR10Dataset():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_shape = (32, 32, 3)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def get_batch(self):\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "        x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
    "        y_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
    "                           [y_train, y_test]]\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def preprocess(self, data, label_data=False):\n",
    "        if label_data:\n",
    "            # convert class vectors to binary class matrices\n",
    "            data = keras.utils.to_categorical(data, self.num_classes)\n",
    "        else:\n",
    "            data = data.astype(\"float32\")\n",
    "            data /= 255  # convert the value to 0~1 scale\n",
    "            shape = (data.shape[0],) + self.image_shape  # add dataset length\n",
    "            data = data.reshape(shape)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, loss, optimizer):\n",
    "        self._target = model\n",
    "        self._target.compile(\n",
    "            loss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "            )\n",
    "        self.verbose = 1\n",
    "        logdir = \"logdir_cifar10_deep_with_aug\"\n",
    "        self.log_dir = os.path.join(os.path.dirname(__file__), logdir)\n",
    "        self.model_file_name = \"model_file.hdf5\"\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
    "        if os.path.exists(self.log_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(self.log_dir)  # remove previous execution\n",
    "        os.mkdir(self.log_dir)\n",
    "\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (0~180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally\n",
    "            height_shift_range=0.1,  # randomly shift images vertically\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "\n",
    "        # compute quantities for normalization (mean, std etc)\n",
    "        datagen.fit(x_train)\n",
    "\n",
    "        # split for validation data\n",
    "        indices = np.arange(x_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        validation_size = int(x_train.shape[0] * validation_split)\n",
    "        x_train, x_valid = \\\n",
    "            x_train[indices[:-validation_size], :], \\\n",
    "            x_train[indices[-validation_size:], :]\n",
    "        y_train, y_valid = \\\n",
    "            y_train[indices[:-validation_size], :], \\\n",
    "            y_train[indices[-validation_size:], :]\n",
    "\n",
    "        model_path = os.path.join(self.log_dir, self.model_file_name)\n",
    "        self._target.fit_generator(\n",
    "            datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "            steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_valid, y_valid),\n",
    "            callbacks=[\n",
    "                TensorBoard(log_dir=self.log_dir),\n",
    "                ModelCheckpoint(model_path, save_best_only=True)\n",
    "            ],\n",
    "            verbose=self.verbose,\n",
    "            workers=4\n",
    "        )\n",
    "\n",
    "\n",
    "dataset = CIFAR10Dataset()\n",
    "\n",
    "# make model\n",
    "model = network(dataset.image_shape, dataset.num_classes)\n",
    "\n",
    "# train the model\n",
    "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
    "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=RMSprop())\n",
    "trainer.train(\n",
    "    x_train, y_train, batch_size=128, epochs=15, validation_split=0.2\n",
    "    )\n",
    "\n",
    "# show result\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-geography",
   "metadata": {},
   "source": [
    "# 予測\n",
    "- sample_imagesというフォルダに予測する画像ファイルを格納しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"logdir_cifar10_deep_with_aug/model_file.hdf5\"\n",
    "images_folder = \"sample_images\"\n",
    "\n",
    "# load model\n",
    "model = load_model(model_path)\n",
    "image_shape = (32, 32, 3)\n",
    "\n",
    "\n",
    "# load images\n",
    "def crop_resize(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    length = min(image.size)\n",
    "    crop = image.crop((0, 0, length, length))\n",
    "    resized = crop.resize(image_shape[:2])  # use width x height\n",
    "    img = np.array(resized).astype(\"float32\")\n",
    "    img /= 255\n",
    "    return img\n",
    "\n",
    "\n",
    "folder = Path(images_folder)\n",
    "image_paths = [str(f) for f in folder.glob(\"*.png\")]\n",
    "images = [crop_resize(p) for p in image_paths]\n",
    "images = np.asarray(images)\n",
    "\n",
    "predicted = model.predict_classes(images)\n",
    "\n",
    "assert predicted[0] == 3, \"image should be cat.\"\n",
    "assert predicted[1] == 5, \"image should be dog.\"\n",
    "\n",
    "print(\"You can detect cat & dog!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
