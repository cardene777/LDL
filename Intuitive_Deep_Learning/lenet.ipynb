{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "significant-phase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/local/lib/python3.7/site-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/local/lib/python3.7/site-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/12\n",
      "48000/48000 [==============================] - 102s 2ms/step - loss: 0.1931 - accuracy: 0.9405 - val_loss: 0.0609 - val_accuracy: 0.9816\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/local/lib/python3.7/site-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/12\n",
      "48000/48000 [==============================] - 102s 2ms/step - loss: 0.0501 - accuracy: 0.9847 - val_loss: 0.0435 - val_accuracy: 0.9878\n",
      "Epoch 3/12\n",
      "48000/48000 [==============================] - 101s 2ms/step - loss: 0.0332 - accuracy: 0.9898 - val_loss: 0.0391 - val_accuracy: 0.9881\n",
      "Epoch 4/12\n",
      "48000/48000 [==============================] - 101s 2ms/step - loss: 0.0252 - accuracy: 0.9916 - val_loss: 0.0339 - val_accuracy: 0.9896\n",
      "Epoch 5/12\n",
      "48000/48000 [==============================] - 101s 2ms/step - loss: 0.0183 - accuracy: 0.9943 - val_loss: 0.0362 - val_accuracy: 0.9910\n",
      "Epoch 6/12\n",
      "48000/48000 [==============================] - 101s 2ms/step - loss: 0.0134 - accuracy: 0.9956 - val_loss: 0.0391 - val_accuracy: 0.9902\n",
      "Epoch 7/12\n",
      "48000/48000 [==============================] - 101s 2ms/step - loss: 0.0105 - accuracy: 0.9965 - val_loss: 0.0369 - val_accuracy: 0.9905\n",
      "Epoch 8/12\n",
      "48000/48000 [==============================] - 101s 2ms/step - loss: 0.0089 - accuracy: 0.9970 - val_loss: 0.0424 - val_accuracy: 0.9883\n",
      "Epoch 9/12\n",
      "48000/48000 [==============================] - 100s 2ms/step - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.0482 - val_accuracy: 0.9893\n",
      "Epoch 10/12\n",
      "48000/48000 [==============================] - 100s 2ms/step - loss: 0.0067 - accuracy: 0.9976 - val_loss: 0.0426 - val_accuracy: 0.9909\n",
      "Epoch 11/12\n",
      "48000/48000 [==============================] - 100s 2ms/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 0.0444 - val_accuracy: 0.9908\n",
      "Epoch 12/12\n",
      "48000/48000 [==============================] - 100s 2ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0477 - val_accuracy: 0.9890\n",
      "Test loss: 0.03852401277199416\n",
      "Test accuracy: 0.9907000064849854\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "def lenet(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # extract image features by convolution and max pooling layers\n",
    "    model.add(Conv2D(\n",
    "        20, kernel_size=5, padding=\"same\",\n",
    "        input_shape=input_shape, activation=\"relu\"\n",
    "        ))   # 5x5のフィルター20個を使用して畳み込み。出力は28x28x20\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))   # 特徴マップのサイズを圧縮。処理後の特徴マップは14x14x20\n",
    "    model.add(Conv2D(50, kernel_size=5, padding=\"same\", activation=\"relu\"))   # 出力は14x14x50\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))   # 出力は7x7x50\n",
    "    # classify the class by fully-connected layers\n",
    "    model.add(Flatten())   # マトリクスのデータである特徴マップをベクトルに変換し、後続の全結合層とつなぐ。\n",
    "    model.add(Dense(500, activation=\"relu\"))   # サイズ500のベクトルを出力。\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))   # 値を0~1の確率値に変換\n",
    "    return model\n",
    "\n",
    "\n",
    "class MNISTDataset():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_shape = (28, 28, 1)  # image is 28x28x1 (grayscale)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def get_batch(self):\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "        x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
    "        y_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
    "                           [y_train, y_test]]\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def preprocess(self, data, label_data=False):\n",
    "        if label_data:\n",
    "            # convert class vectors to binary class matrices\n",
    "            data = keras.utils.to_categorical(data, self.num_classes)\n",
    "        else:\n",
    "            data = data.astype(\"float32\")\n",
    "            data /= 255  # convert the value to 0~1 scale\n",
    "            shape = (data.shape[0],) + self.image_shape  # add dataset length\n",
    "            data = data.reshape(shape)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, loss, optimizer):\n",
    "        self._target = model\n",
    "        self._target.compile(\n",
    "            loss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "            )\n",
    "        self.verbose = 1\n",
    "        logdir = \"logdir_lenet\"\n",
    "        self.log_dir = os.path.join(os.path.dirname('__file__'), logdir)\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
    "        if os.path.exists(self.log_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(self.log_dir)  # remove previous execution\n",
    "        os.mkdir(self.log_dir)\n",
    "\n",
    "        self._target.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=batch_size, epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[TensorBoard(log_dir=self.log_dir)],\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "\n",
    "dataset = MNISTDataset()\n",
    "\n",
    "# make model\n",
    "model = lenet(dataset.image_shape, dataset.num_classes)\n",
    "\n",
    "# train the model\n",
    "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
    "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=Adam())\n",
    "trainer.train(\n",
    "    x_train, y_train, batch_size=128, epochs=12, validation_split=0.2\n",
    "    )\n",
    "\n",
    "# show result\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-diagram",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
